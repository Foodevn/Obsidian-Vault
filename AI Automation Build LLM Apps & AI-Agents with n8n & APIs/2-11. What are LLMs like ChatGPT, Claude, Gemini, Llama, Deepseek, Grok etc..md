# Ghi chú học tập: Mô hình ngôn ngữ lớn (LLM) và Token

## Khái niệm về LLM (Large Language Model)

### Định nghĩa
- **LLM (Large Language Model)**: Mô hình ngôn ngữ lớn, là "bộ não" của [[AI Automation]] và [[AI Agents]], xử lý ngôn ngữ tự nhiên để thực hiện các tác vụ như tạo văn bản, trả lời câu hỏi, hoặc phân tích dữ liệu.
- **Ví dụ**: [[ChatGPT]], [[Grok]], [[Gemini]], [[LLaMA]], [[Claude]], [[DeepSeek]], [[Grok]] (từ xAI).

### Cấu trúc của LLM
- Một LLM bao gồm **hai tệp chính**:
  - **Parameter File (Tệp tham số)**:
    - Chứa các tham số (parameters) được huấn luyện từ dữ liệu văn bản.
    - Ví dụ: Mô hình [[LLaMA 2]] 7B có **70 tỷ tham số**, được nén thành tệp ~140GB từ 10TB dữ liệu văn bản (Wikipedia, website, v.v.).
    - Tương tự như tệp nén ZIP, yêu cầu nhiều sức mạnh GPU để huấn luyện và nén dữ liệu.
  - **Run File (Tệp thực thi)**:
    - Mã nguồn (~500 dòng, thường viết bằng C hoặc Python) để chạy tệp tham số.
    - Cho phép thực thi mô hình trên máy cục bộ hoặc qua API.

### Quá trình huấn luyện LLM
1. **Pre-training (Huấn luyện trước)**:
   - Sử dụng nhiều GPU để nén lượng lớn dữ liệu văn bản (ví dụ: 10TB) vào tệp tham số.
   - Mô hình học cách dự đoán từ tiếp theo dựa trên cấu trúc văn bản (tương tự như "hallucination" - tạo văn bản tự nhiên).
2. **Fine-tuning (Tinh chỉnh)**:
   - Cung cấp khoảng 100,000 ví dụ câu hỏi và câu trả lời để mô hình học cách trả lời theo cách con người mong muốn.
   - Ví dụ: Câu hỏi "What should I eat today?" → Trả lời "You could eat steak today."
   - Ít tốn tài nguyên GPU hơn pre-training.
3. **Reinforcement Learning (Học tăng cường)**:
   - Đánh giá chất lượng câu trả lời (thumbs up/down) để cải thiện mô hình.
   - Giúp mô hình điều chỉnh theo sở thích người dùng.

### Mở nguồn và đóng nguồn
- **Open-source LLMs (Mô hình mã nguồn mở)**:
  - Ví dụ: [[LLaMA 2]], [[LLaMA 3]].
  - Ưu điểm: Có thể tải parameter file và run file, chạy cục bộ trên máy tính → **tối đa bảo mật dữ liệu**, không cần kết nối internet.
  - Nhược điểm: Yêu cầu phần cứng mạnh (GPU) để chạy hiệu quả.
- **Closed-source LLMs (Mô hình đóng nguồn)**:
  - Ví dụ: [[ChatGPT]], [[Claude]].
  - Nhược điểm: Chỉ sử dụng qua giao diện web hoặc API, không thể tải tệp về để chạy cục bộ.

### Transformer Architecture
- **Cơ chế**: LLM sử dụng kiến trúc transformer (mạng nơ-ron) để xử lý văn bản.
- **Cách hoạt động**:
  - Chuyển đổi văn bản thành số (token) để mạng nơ-ron tính toán.
  - Dự đoán từ tiếp theo dựa trên xác suất (next-word prediction).

## Token trong LLM

### Định nghĩa
- **Token**: Đơn vị nhỏ nhất của văn bản mà LLM xử lý, thường tương ứng với một từ, dấu câu, hoặc một phần của từ.
- **Ví dụ**:
  - Câu "What can I eat today?" có 5 token, ~20 ký tự.
  - Từ "invisible" có thể được chia thành 2 token (in-visi-ble).
- **Cách hoạt động**:
  - Văn bản được chuyển thành token ID (số) để mạng nơ-ron tính toán.
  - Ví dụ: "What can I eat today?" → Token ID: [123, 456, 789, ...].
  - LLM sử dụng token ID để dự đoán từ tiếp theo.

### Giới hạn token (Token Limit)
- **Ý nghĩa**: Mỗi LLM có giới hạn số token có thể xử lý trong một phiên (context window).
- **Ví dụ giới hạn**:
  - [[GPT-4 Turbo]], [[GPT-4 Omni]]: ~128,000 token (~100,000 từ).
  - Một số mô hình mã nguồn mở nhỏ: ~4,000 token.
  - Mô hình tiên tiến: Lên đến 2 triệu token.
- **Hậu quả**:
  - Khi vượt quá giới hạn token, LLM quên thông tin trước đó (chỉ nhớ các token gần nhất).
  - Ví dụ: Trong [[ChatGPT]], nếu bạn hỏi về "a fox story", sau đó hỏi thêm nhiều câu hỏi khác, LLM có thể quên câu chuyện ban đầu khi vượt quá 128,000 token.
- **Giải pháp**:
  - Sử dụng [[Vector Database]] hoặc kỹ thuật khác để lưu trữ ngữ cảnh dài hạn (sẽ được đề cập sau).
  - Tối ưu câu hỏi để giảm số token sử dụng.

### Chi phí token
- **Closed-source LLMs**: 
  - Sử dụng API (như [[OpenAI API]]) yêu cầu trả phí dựa trên số token xử lý (cả input và output).
  - Ví dụ: 1,500 từ ~ 2,048 token (theo tỷ lệ trung bình 4 ký tự/token của OpenAI).
- **Open-source LLMs**:
  - Chạy cục bộ, không tốn phí token, chỉ cần chi phí phần cứng (GPU).

## Tầm quan trọng
- **Hiểu LLM**: Giúp nắm rõ cách AI agents và tự động hóa AI hoạt động, đặc biệt khi tích hợp với [[n8n]] hoặc [[Flowise]].
- **Hiểu token**: Cần thiết để quản lý chi phí khi sử dụng API và tối ưu hóa quy trình trong giới hạn token.

## Ghi chú bổ sung
- **Liên kết nội bộ**:
  - [[LLM]], [[ChatGPT]], [[Grok]], [[LLaMA]], [[OpenAI API]], [[Vector Database]], [[Prompt Engineering]].
- **Tài nguyên tham khảo**:
  - Xem thêm về [[OpenAI API]] và chi phí token trong bài giảng tiếp theo.
  - Tài liệu về [[LLaMA]] trên GitHub hoặc tài liệu OpenAI về token.
- **Lưu ý**:
  - Các khái niệm về [[Prompt Engineering]] sẽ được giải thích chi tiết sau.
  - Nếu đã quen thuộc với LLM, có thể bỏ qua bài giảng này và chuyển sang phần thực hành.

---

**Định dạng Markdown**: Ghi chú được tối ưu cho Obsidian, dễ đọc trên cả máy tính và di động. Các thuật ngữ chuyên ngành được giữ nguyên tiếng Anh trong ngoặc để dễ tra cứu.